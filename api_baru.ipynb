{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gGLgqaVYabl",
        "outputId": "88349dba-5f73-470b-d64d-6bea18ce35aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (0.123.10)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.12/dist-packages (0.38.0)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.5.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.50.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from fastapi) (2.12.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from fastapi) (4.15.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi) (0.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (8.3.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi) (4.12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.6.2->starlette<0.51.0,>=0.40.0->fastapi) (3.11)\n",
            "Downloading pyngrok-7.5.0-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KX9WvSLKYrcm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTo-uEBT5Uqx"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY8r-sS85a1n"
      },
      "outputs": [],
      "source": [
        "%%writefile preprocessing.py\n",
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# KONFIG\n",
        "MAX_LEN = 30\n",
        "\n",
        "# PATH KAMUS SLANG (CSV)\n",
        "KAMUS_PATH = \"/content/drive/MyDrive/Colab Notebooks/Deep learning/TUBES/nonbaku_gabungan_kamus.csv\"\n",
        "\n",
        "# LOAD SLANG DICTIONARY\n",
        "def load_slang_dictionary(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Kamus slang tidak ditemukan: {path}\")\n",
        "\n",
        "    df = pd.read_csv(path)\n",
        "    df.columns = df.columns.str.lower().str.strip()\n",
        "\n",
        "    if 'slang' not in df.columns or 'formal' not in df.columns:\n",
        "        raise ValueError(\"CSV harus punya kolom 'slang' dan 'formal'\")\n",
        "\n",
        "    slang_dict = dict(\n",
        "        zip(\n",
        "            df['slang'].astype(str),\n",
        "            df['formal'].astype(str)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(f\"âœ… Slang dictionary loaded: {len(slang_dict)} entries\")\n",
        "    return slang_dict\n",
        "\n",
        "\n",
        "slang_dict = load_slang_dictionary(KAMUS_PATH)\n",
        "\n",
        "# FINAL PREPROCESSING FUNCTION\n",
        "def preprocess_complete(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # hapus token username & URL\n",
        "    text = re.sub(r\"<username>\", \" \", text)\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \" \", text)\n",
        "\n",
        "    # hapus mention & hashtag\n",
        "    text = re.sub(r\"@\\w+\", \" \", text)\n",
        "    text = re.sub(r\"#\", \" \", text)\n",
        "\n",
        "    # normalisasi elongasi (anjiiirrr â†’ anjir)\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
        "\n",
        "    # hapus karakter non huruf\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "\n",
        "    # rapikan spasi\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # normalisasi slang\n",
        "    words = text.split()\n",
        "    words = [slang_dict.get(w, w) for w in words]\n",
        "\n",
        "    return \" \".join(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFLwwaijY-pY"
      },
      "source": [
        "#API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNhIShb8Y87G"
      },
      "outputs": [],
      "source": [
        "%%writefile api.py\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "from preprocessing import preprocess_complete, MAX_LEN\n",
        "\n",
        "\n",
        "# LOAD MODEL\n",
        "MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/Deep learning/TUBES/Model/denganfasttext.h5\"\n",
        "TOKENIZER_PATH = \"/content/drive/MyDrive/Colab Notebooks/Deep learning/TUBES/Model/tokenizer (3).pkl\"\n",
        "\n",
        "model = tf.keras.models.load_model(MODEL_PATH)\n",
        "\n",
        "with open(TOKENIZER_PATH, \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "\n",
        "# FASTAPI INIT\n",
        "app = FastAPI(\n",
        "    title=\"Cyberbullying Detection API\",\n",
        "    description=\"API untuk deteksi cyberbullying menggunakan BiLSTM\",\n",
        "    version=\"1.0\"\n",
        ")\n",
        "\n",
        "class TextInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\n",
        "        \"status\": \"API running\"\n",
        "    }\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(data: TextInput):\n",
        "    try:\n",
        "        raw_text = data.text\n",
        "        clean_text = preprocess_complete(raw_text)\n",
        "\n",
        "        if not clean_text.strip():\n",
        "            return {\n",
        "                \"input\": raw_text,\n",
        "                \"label\": \"unknown\",\n",
        "                \"score\": 0.0,\n",
        "                \"note\": \"Empty after preprocessing\"\n",
        "            }\n",
        "\n",
        "        seq = tokenizer.texts_to_sequences([clean_text])\n",
        "        pad = pad_sequences(seq, maxlen=MAX_LEN, padding=\"post\")\n",
        "\n",
        "        score = float(model.predict(pad, verbose=0)[0][0])\n",
        "        label = \"cyberbullying\" if score > 0.5 else \"non-cyberbullying\"\n",
        "\n",
        "        return {\n",
        "            \"input\": raw_text,\n",
        "            \"clean_text\": clean_text,\n",
        "            \"label\": label,\n",
        "            \"score\": score\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": str(e)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJrsg0bcZGr8"
      },
      "source": [
        "#Run ngrok+api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CHS64t_VZFw8",
        "outputId": "ea6dd842-e2b3-48d1-ef4f-a222afc1cacc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸš€ Public URL: NgrokTunnel: \"https://nongregariously-unabdicative-kym.ngrok-free.dev\" -> \"http://localhost:8000\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import uvicorn\n",
        "\n",
        "\n",
        "# SET TOKEN\n",
        "ngrok.set_auth_token(\"362ocoEjtgGftMd2Z8bWNbBUkQh_3jVryS1qitHi7UEdsHfuK\")\n",
        "PORT = 8000\n",
        "\n",
        "\n",
        "# OPEN NGROK TUNNEL\n",
        "public_url = ngrok.connect(PORT)\n",
        "print(\"ðŸš€ Public URL:\", public_url)\n",
        "\n",
        "\n",
        "# RUN FASTAPI\n",
        "def run():\n",
        "    uvicorn.run(\"api:app\", host=\"0.0.0.0\", port=PORT, log_level=\"info\")\n",
        "\n",
        "threading.Thread(target=run).start()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}